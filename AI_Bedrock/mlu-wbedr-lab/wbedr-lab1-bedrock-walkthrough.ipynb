{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU Getting Started with Bedrock and Prompt Engineering</a>\n",
    "## <a name=\"0\">Amazon Bedrock API Walkthrough</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "1. Before running this notebook, make sure you have already granted access to the Bedrock model(s) used here, by following the instructions in the `Lab Setup` session at the course portal.\n",
    "\n",
    "2. The Titan models undergo regular updates. Therefore, the returned results in this notebook may be different from the video recordings provided by MLU for this course.\n",
    "\n",
    "---\n",
    "    \n",
    "Amazon Bedrock is a fully managed service that makes foundation models (FMs) from Amazon and third-party model providers easily accessible through an API. This notebook covers the Amazon Bedrock API using SDK for Python (Boto3).\n",
    "\n",
    "Topics:\n",
    "\n",
    "* __Accessing the Bedrock service using the SDK for Python (Boto3)__\n",
    "* __Performing Bedrock API call: Basic API call and different options for customization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### 1. Accessing the Bedrock service using the SDK for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We can access the Bedrock service through boto3 by providing the service name, region name and endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "bedrock = session.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Let's take a look at the available Large Language Models (LLMs) in Bedrock. As the list is long, here we print just the first three models, but you can print all by removing `[0:3]` form the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock.list_foundation_models()[\"modelSummaries\"][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We use the service name 'bedrock-runtime' for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For inference\n",
    "bedrock_inference = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Model IDs can be used to select a specific model in the API calls. \n",
    "\n",
    "This demo is focusing on the __Amazon Titan Text G1 - Premier__ model from __Amazon__. We will use the model with id: `amazon.titan-text-premier-v1:0`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 2. Bedrock API Call\n",
    "\n",
    "Bedrock API has the following parameters:\n",
    "* __`body`:__ The message body that includes the input text and model parameters. Model parameters help customize the models and the generated outputs.\n",
    "* __`modelId`:__ Identifier of the model. We can pick a model from the list of models printed in the previous code block.\n",
    "* __`accept`:__ The desired type of the inference body in the response.\n",
    "* __`contentType`:__ The type of the input data in the request body.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Model parameters:__\n",
    "\n",
    "These parameters are provided within the body of the API call. Basically, we can control the randomness and the length of the generated sequences. \n",
    "\n",
    "__Randomness:__ \n",
    "\n",
    "* __`temperature`:__ Controls the randomness of the generated sequences. This parameter is between zero and one. When set closer to zero, the model tends to select higher probability words. When set further away from zero, the model may select lower-probability words. Temperature of zero gives the same output for the same input at every run.\n",
    "* __`topP`:__ Top P defines a cut-off based on the sum of probabilities of the potential choices. With this cut-off, the model only selects from the most probable words whose probabilities sum up to the topP value. \n",
    "\n",
    "__Length:__ \n",
    "\n",
    "* __`maxTokens`:__ Controls the maximum number of tokens in the generated response.\n",
    "* __`stopSequences`:__ A sequence of characters to stop the model from generating its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's define a function that will build our prompt as well as pass some model parameters to Bedrock. Once the call parameters are ready, we can use the __invoke_model()__ function to send the data and collect the response from the model. Then, we return the response at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(prompt_data, temperature=0.0, top_p=1.0, max_token_count=1000):\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputText\": prompt_data,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p,\n",
    "                \"maxTokenCount\": max_token_count,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    modelId = \"amazon.titan-text-premier-v1:0\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_inference.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"results\"][0][\"outputText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's construct a simple API call and run it. \n",
    "\n",
    "As a sample text input, we use the following: __\"Can you name a few real-life applications of natural language processing?\"__. \n",
    "\n",
    "For inference parameters, we set the __`temperature`__ to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(Markdown(send_prompt(prompt_data, temperature=0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "This provides a list of real-life NLP applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "If we want to generate slightly different looking outputs, we can increase the __`temperature`__ value. Let's also set the __`maxTokens`__ parameter this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Changing the temperature parameter will create a slightly different looking list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(Markdown(send_prompt(prompt_data, temperature=0.5, max_token_count=450)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "In addition to the __`temperature`__ and __`maxTokens`__ parameters, we can also add in the __`topP`__ parameter to set a cut-off for the sum of the probabilities of the potential words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(\n",
    "    Markdown(send_prompt(prompt_data, max_token_count=450, temperature=0.5, top_p=0.7))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}