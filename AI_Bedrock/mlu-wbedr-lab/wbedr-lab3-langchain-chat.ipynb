{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU Getting Started with Bedrock and Prompt Engineering</a>\n",
    "## <a name=\"0\">Amazon Bedrock with LangChain</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "1. Before running this notebook, make sure you have already granted access to the Bedrock model(s) used here, by following the instructions in the `Lab Setup` session at the course portal.\n",
    "\n",
    "2. The Titan models undergo regular updates. Therefore, the returned results in this notebook may be different from the video recordings provided by MLU for this course.\n",
    "\n",
    "---\n",
    "    \n",
    "[__LangChain__](https://python.langchain.com/docs/introduction/) is a popular open source framework to develop applications with Large Language Models. Recent versions of LangChain started to support Amazon Bedrock models.\n",
    "\n",
    "In this notebook, we will build __a simple chatbot__ using Amazon's __Amazon Titan Text G1 - Premier__ with Amazon Bedrock. Let's start!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Imports and API calls \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We import the Bedrock module first and use the __Amazon Titan Text G1 - Premier__ from it. We can pass model parameters at this point. For example we set the temperature to zero and maximum number of tokens in the text response to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.0, \"maxTokenCount\": 500},\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We can also set a certain prompt template. Below, we create a slightly different version of the default template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Question: {input}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Once we have the LLM and the prompt template, we can start a conversation chain. Inside the function, we provide the LLM and set a few other parameters. \n",
    "* __verbose__ prints the conversation history during the chat\n",
    "* __memory__ is responsible for storing the conversation history.\n",
    "* Inside __ConversationBufferMemory()__, we can also set different names for the AI and user through __ai_prefix__ and __human_prefix__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Answer\", human_prefix=\"Question\"),\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's print out our conversation template. This is a general template for conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "default_prompt_template = conversation.prompt.template\n",
    "\n",
    "Markdown(default_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Let's start the conversation!  We send our message by calling the __predict()__ function with our text. As we set the __verbose__ parameter __true__ earlier, history of the conversation will be printed out first. Then, we will see the response of the chatbot.\n",
    "\n",
    "### First message\n",
    "\n",
    "Let's start with asking the AI if they are familiar with machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"Hello! Are you familiar with Machine Learning?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Second message\n",
    "\n",
    "Next, we are specifically interested in LLMs. Let's ask about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    conversation.predict(\n",
    "        input=\"Can you list a few applications of Large Language Models?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Third message\n",
    "\n",
    "We ask about some recent developments in the field to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"What are some recent developments in LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### The last message\n",
    "\n",
    "At the end, we thank and end the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"Nice. Thanks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}