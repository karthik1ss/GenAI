{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# <a name=\"0\">MLU Getting Started with Bedrock and Prompt Engineering</a>\n",
    "## <a name=\"0\">Prompt Engineering</a>\n",
    "\n",
    "<br>\n",
    "    \n",
    "---    \n",
    "### Important Notes\n",
    "\n",
    "1. Before running this notebook, make sure you have already granted access to the Bedrock model(s) used here, by following the instructions in the `Lab Setup` session at the course portal.\n",
    "\n",
    "2. The Titan models undergo regular updates. Therefore, the returned results in this notebook may be different from the video recordings provided by MLU for this course.\n",
    "\n",
    "---\n",
    "    \n",
    "In this notebook, we will explain the concept of prompt engineering using some common use cases. \n",
    "\n",
    "__What is prompt engineering?__\n",
    "\n",
    "We can improve the quality of the generated responses by constructing (or engineering) the prompts in different ways. We call this process __prompt engineering__. This is usually an __iterative process__ and it can take a few attempts to find the best spot for your problem. \n",
    "\n",
    "We can list a few suggestions to help you construct better prompts:\n",
    "* __Write clear and specific instructions.__\n",
    "* __Highlight or specify the part of the prompt where the model should execute on.__\n",
    "* __Add details or restrictions to your prompt.__\n",
    "* __If the problem requires executing multiple steps, instruct the model to follow a step-by-step approach.__\n",
    "\n",
    "Finding the optimum prompts is an iterative process. You may need to run some experiments and measure the quality of the generated outputs.\n",
    "\n",
    "\n",
    "__Example Problems:__\n",
    "\n",
    "We will focus on some common ML tasks with the __Amazon Titan Text G1 - Premier__. We instruct the model through the prompt messages, so pay attention to how we construct those messages. \n",
    "\n",
    "These are the tasks we cover:\n",
    "* __Text summarization__\n",
    "* __Question Answering__\n",
    "* __Text Generation__\n",
    "* __In-context learning: Zero-shot, one-shot, few-shot learning__\n",
    "* __Chain of thought concept__\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We access the Bedrock service through boto3 by providing the service name, region name and endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "bedrock_inference = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Let's specify the API parameters. We will use the __Amazon Titan Text G1 - Premier__ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(prompt_data, temperature=0.0, top_p=0.5, max_token_count=1000):\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputText\": prompt_data,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p,\n",
    "                \"maxTokenCount\": max_token_count,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    modelId = \"amazon.titan-text-premier-v1:0\"\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_inference.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"results\"][0][\"outputText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Example problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 1. Text summarization:\n",
    "\n",
    "With text summarization, the main purpose is to create a shorter version of a given text while preserving the relevant information in it. \n",
    "\n",
    "We use the following text from the [sustainability section](https://sustainability.aboutamazon.com/environment/renewable-energy) of [about.amazon.com](https://www.aboutamazon.com/).\n",
    "\n",
    "\n",
    "<p style=\"font-size:12pt;\">\n",
    "    \"In 2021, we reached 85% renewable energy across our business. Our first solar projects in South Africa and the United Arab Emirates came online, and we announced new projects in Singapore, Japan, Australia, and China. Our projects in South Africa and Japan are the first corporate-backed, utility-scale solar farms in these countries. We also announced two new offshore wind projects in Europe, including our largest renewable energy project to date. As of December 2021, we had enabled more than 3.5 gigawatts of renewable energy in Europe through 80 projects, making Amazon the largest purchaser of renewable energy in Europe.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Let's start with the first summarization example below. We pass this text as well as the instruction to summarize it. The instruction part of the prompt becomes __The following is a text about Amazon. Summarize this:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"The following is a text about Amazon. Summarize this:  \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Nice. This text is shorter. We can set the desired lenght of the summary by adding more constraints to the instructions. Let's create a one-sentence summary of this text. The instruction part of the prompt becomes the following: __Summarize it in one sentence.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"The following is a text about Amazon. Summarize it in one sentence. \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Nice! The model generated one sentence summary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2. Question Answering:\n",
    "\n",
    "In Question Answering problem, a Machine Learning model answers some questions using some provided context. Here as context, we will use the previous text about Amazon's sustainability efforts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "The first question is asking about the names of the countries mentioned in the text. \n",
    "\n",
    "The instruction section of the prompt is __What are the names of the countries in the following text?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"What are the names of the countries in the following text? \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Nice. We get all of the geographical places mentioned in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Let's try to learn something specific about the document. For example, the amount of gigawatts that the project in Europe enabled. \n",
    "\n",
    "The instruction section of the prompt is __How many gigawatts of energy did Amazon enable in Europe according to the following text?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many gigawatts of energy did Amazon enable in \\\n",
    "Europe according to the following text? \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Nice. We were able to extract that information and return in the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Let's try another example, this time without context. Context information may not be necessary for some questions. For example, we can ask some general questions like below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "__How many months are there in a year?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many months are there in a year?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "__How many meters are in a mile?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many meters are in a mile?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "__What is the result when you add up 2 and 9?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"What is the result when you add up 2 and 9?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer is correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 3. Text Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Text generation is one of the common use cases for Large Language Models. The main purpose is to generate some high quality text considering a given input. We will cover a few examples here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "__Customer service example:__\n",
    "\n",
    "Let's start with a customer feedback example. Assume we want to write an email to a customer who had some problems with a product that they purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "__Write an email response from Any company customer service \\\n",
    "based on the following email that was received from a customer__\n",
    "\n",
    "__Customer email: \"I am not happy with this product. I had a difficult \\\n",
    "time setting it up correctly because the instructions do not cover all \\\n",
    "the details. Even after the correct setup, it stopped working after \\\n",
    "a week.\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Write an email response from Any company customer service \\\n",
    "based on the following email that was received from a customer\n",
    "\n",
    "Customer email: \"I am not happy with this product. I had a difficult \\\n",
    "time setting it up correctly because the instructions do not cover all \\\n",
    "the details. Even after the correct setup, it stopped working after \\\n",
    "a week.\" \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Nice! The generated text asks customer to provide more details to resolve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "__Generating product descriptions:__\n",
    "\n",
    "We can use generative ai to write creative product descriptions for our products. In the example below, we create three product descriptions for a sunglasses product.\n",
    "\n",
    "__Product: Sunglasses.  \\\n",
    "Keywords: polarized, style, comfort, UV protection. \\\n",
    "Create three variations of a detailed product \\\n",
    "description for the product listed above, each \\\n",
    "variation of the product description must \\\n",
    "use at least two of the listed keywords.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Product: Sunglasses.  \\\n",
    "Keywords: polarized, style, comfort, UV protection. \\\n",
    "List three different product descriptions \\\n",
    "for the product listed above using \\\n",
    "at least two of the provided keywords.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 4. In-context learning: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "As pre-trained large language models learn from large and diverse data sources, they tend to build a holistic view of languages and text. This advantage allows them to learn from some input-output pairs that they are presented within the input texts. \n",
    "\n",
    "In this section, we will explain this __\"in-context\"__ learning capability with some examples. Depending on the level of information presented to the model, we can use zero-shot, one-shot or few-shot learning. We start with the most extreme case, no information presented to the model. This is called __\"zero-shot-learning\"__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "#### Zero-shot learning:\n",
    "Assume the model is given a translation task and an input word.\n",
    "\n",
    "__Translate English to Spanish \\\n",
    " cat ==>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Translate the following word from English to Spanish \\\n",
    "word: cat \\\n",
    "translation: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Correctly translated to Spanish. Let's try something different in the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### One-shot learning:\n",
    "We can give the model one example and let it learn from the example to solve a problem. Below, we provide an example sentence about a cat and the model completes the second sentence about a table in a similar way.\n",
    "\n",
    "__Answer the last question \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is an animal \\\n",
    "\\##  \\\n",
    "last question: what is a car?\\\n",
    "answer: car is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is an animal \\\n",
    "## \\\n",
    "last question: what is a car? \\\n",
    "answer: car is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "It worked very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "#### Few-shot learning:\n",
    "We can give the model multiple examples to learn from. Providing more examples can help the model produce more accurate results. Let's also change the style of the example answers by adding some __negation__ to them.\n",
    "\n",
    "__Answer the last question \\\n",
    "question: what is a car? \\\n",
    "answer: car is not an animal \\\n",
    "\\## \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is not a vehicle \\\n",
    "\\## \\\n",
    "last question: what is a shoe? \\\n",
    "answer: shoe is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question\n",
    "question: what is a car?\n",
    "answer: car is not an animal\n",
    "##\n",
    "question: what is a cat?\n",
    "answer: cat is not a vehicle\n",
    "##\n",
    "last question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The response picked up the overall style very well. See that it responded starting with \"not\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "We can increase the __temperature__ to get different responses. Let's try that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question\n",
    "question: what is a car?\n",
    "answer: car is not an animal\n",
    "##\n",
    "question: what is a cat?\n",
    "answer: cat is not a vehicle\n",
    "##\n",
    "last question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, top_p=1.0, temperature=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Let's try one more example. This time we remove the instruction and try to complete the last sentence.\n",
    "\n",
    "__question: what is a cat? \\\n",
    "answer: cat is a domesticated wild animal that belongs to the Felidae family. \\\n",
    "\\##  \\\n",
    "question: what is a car? \\\n",
    "answer: car is a vehicle with wheels that is used for transportation. \\\n",
    "\\##  \\\n",
    "last question: what is a shoe?\\\n",
    "answer: shoe is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"\n",
    "question: what is a cat?\n",
    "answer: cat is a domesticated wild animal that belongs to the Felidae family.\n",
    "##\n",
    "question: what is a car?\n",
    "answer: car is a vehicle with wheels that is used for transportation.\n",
    "##\n",
    "question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "It worked again. The model nicely followed the provided pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### 5. Chain of thought concept: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Chain of thought concept breaks down a problem into a series of intermediate reasoning steps. This way of thinking has significantly improved the quality of the outputs generated by the Large Language Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Let's start with a simple problem. Although many problems may seem easy to us, some may be challenging to LLMs if they require solving intermediate steps before giving the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Here is the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "__Answer the following question.__\n",
    "\n",
    "__Question: When I was 16, my sister was half of my age.__ \\\n",
    "__Now, I\u2019m 42. How old is my sister now?__ \\\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the following question.\n",
    "\n",
    "Question: When I was 16, my sister was half of my age. \\\n",
    "Now, I\u2019m 42. How old is my sister now?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "The answer is __incorrect__! This is not a big surprise. Many Large Language Models make these types of mistakes. In this case, the model skipped a few steps to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Let's try another idea. As we have seen in the __in-context__ learning topic, LLMs tend to learn from the provided inputs and apply those learnings to another problems. Here, we will first provide the step by step solution for the problem with different numbers and then ask the model to solve the original problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "__Answer the following question:__ \n",
    "\n",
    "__Question: When I was 10, my sister was half of my age.__ \\\n",
    "__Now, I\u2019m 70. How old is my sister now?__\n",
    "\n",
    "__Answer: When I was 10 years old, my sister was half of my age.__ \\\n",
    "__So, the age of the sister at that time = 10/2 = 5__ \\\n",
    "__This implies that the sister is 5 years younger.__ \\\n",
    "__Now, when I\u2019m 70 years and age of sister = 70 - 5__ \\\n",
    "__Age of sister = 65.__\n",
    "\n",
    "__Question: When I was 16, my sister was half of my age.__ \\\n",
    "__Now I\u2019m 42. How old is my sister now?__\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the following question.\n",
    "\n",
    "Question: When I was 10, my sister was half of my age. \\\n",
    "Now, I\u2019m 70. How old is my sister now?\n",
    "\n",
    "Answer: When I was 10 years old, my sister was half of my age. \\\n",
    "So, the age of the sister at that time = 10/2 = 5 \\\n",
    "This implies that the sister is 5 years younger. \\\n",
    "Now, when I\u2019m 70 years and age of sister = 70 - 5 \\\n",
    "Age of sister = 65. \\\n",
    "\n",
    "Question: When I was 16, my sister was half of my age. \\\n",
    "Now I\u2019m 42. How old is my sister now?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "The model followed the given example and applied the same steps to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"images/mlu-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}